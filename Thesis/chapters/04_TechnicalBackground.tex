\chapter{Technical Background} \label{cpt-technical-background}

In order to put this work in context, some basic knowledge about computer graphics is expected.
The reader is assumed to be familiar with the basic rendering pipeline and how 3D data is 
transformed into a projected 2D image. Techniques like \emph{z-buffering}, \emph{mipmapping}, 
the use of \emph{textures}, basic \emph{transformations}, and \emph{compute shaders} are mentioned 
but not explained in further detail. Beyond that, a more in-depth discussion of relevant techniques 
is presented below.

\section{Voxel Scene Layout} \label{sec-voxel-scene-layout}

The voxel rendering in this work relies on a volumetric representation of the scene.
It is particularly important that the voxel model not only includes the surface but also 
the space within the model. This constraint is often used in interactable use cases, where 
the model is split, cut, or manipulated in any given way. \emph{Minecraft} (Mojang \cite{Mojang2024}, 
2011) lets the player take full control over the sandbox environment and allows adding or 
subtracting voxels as they wish. Essentially, the voxel data of every possible voxel within 
the playable space needs to be somehow present or encoded in memory, even though just a 
fraction of the voxels are actually drawn to the screen.


\subsection*{Octree Data Structure} \label{subsec-octree-data-structure}

One approach to store volumetric voxel data is to use a spatial container to optimize traversal. 
The use of an \emph{octree} incorporates the relevant scene space and subdivides it into smaller 
child nodes. When inserting data into the octree, the position is evaluated, and the data is 
being stored as a \emph{payload} in a node, which includes the given position. If a node holds 
more payload instances than a specified threshold allows, it is split into eight child nodes, 
and the payload originally present in the node is distributed onto the child nodes. This process 
is repeated until all data is finally inserted into the octree. The resulting octree maintains 
all the relevant payload data, having a deeper tree hierarchy where more data is present and a 
shallow hierarchy where (almost) no data is present.

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/octree.jpg}
    \caption{An octree data structure. It stores data alongside its spatial characteristics, relative to 
    other data entries \cite{Six2021}.}
    \label{fig:octree}
\end{figure}

\noindent
Octrees like this can vary in their specific implementation, especially in how they maintain the 
payload. Some might store it right next to the octree node description, others might want to separate 
octree and payload data. Octree nodes could also be appended to a dedicated buffer and therefore be stored 
separately from the tree hierarchy description. Such an approach would maintain cache coherency during an 
unordered traversal of all octree nodes.\\

\noindent
When using hierarchical data structures, dedicated vertex positions can be omitted, and instead, 
the voxel position in space can be implicitly calculated from the hierarchy and the root node's bounds. 
This allows for improvement of memory occupation, which is vital for loading and storing high-resolution 
voxel scenes. A voxel's state can be encoded into either visible (1) or empty (0). A bitwise representation 
of voxels can significantly decrease the memory footprint, which is a techniqe the next approach expands on.

[@TODO: Maybe shorten due to redundancy]
\subsection*{High Resolution Sparse Voxel Directed Acyclic Graphs} \label{subsec-highres-svo-dags}

Kämpe et al. \cite{Kampe2013} use data redundancy to further generalize and optimize high-resolution 
\ac{SVO}s. In their implementation, the octree data structure relies on bitmasks to encode the existence of child 
nodes. This way, empty nodes are ignored, and the information can be easily stored within one byte. This child node 
mask uniquely identifies the layout of the nodes below and can also be used to store the voxel occupation of the 
leaf nodes. Next, they make use of the fact that an octree node's position is uniquely identified by the path along 
the nodes from root to leaf node. Since voxels can be encoded as either being present or not, and non-present 
voxels can be ignored, the data can be significantly reduced in size. Furthermore, equal child node masks 
can simply be shared between different parent nodes instead of being duplicated. Kämpe et al. \cite{Kampe2013} 
suggest a bottom-up approach to remove redundant nodes and replace whole subtrees of the octree by pointing to one 
instance of the same pattern within the tree, which is shown in Figure \ref{fig:sparse-voxel-dag-creation}. 
This optimization can be recursively done to each layer in the tree hierarchy, resulting in an optimized version 
of the data structure. The resulting structure is a more general hierarchical data structure, namely a \ac{DAG}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/highres-sv-dag.jpg}
    \caption{Reducing a \ac{SVO} using the approach of Kämpe et al. \cite{Kampe2013}.}
    \label{fig:sparse-voxel-dag-creation}
\end{figure}

\noindent
This approach seems to be the best solution for reading, storing, and optimizing octree data, although the work
presented in Chapter \ref{cpt-implementation} uses a more basic approach in the implementation. Nevertheless, 
this concept of optimizing voxel data is compatible with the use case presented here and is highly recommended.


\section{Voxel Rendering} \label{sec-voxel-rendering}
[@TODO: Redundant]

\noindent
\emph{Voxel Rendering} refers to the use of primitive objects to represent data elements in a three-dimensional 
grid. It is used to approximate 3D volumes analogous to how 2D pixels approximate shapes, lines, or areas in a 
two-dimensional grid \cite{MegaVoxels2023}. Commonly, uniform cubes are used to represent the data in space, 
which results in a distinct, blocky look, depending on the size of the primitives. Games like \emph{Minecraft} 
(Mojang \cite{Mojang2024}, 2011) popularized this technique for games, which is often used because of its simplicity and low 
memory footprint. Voxels have been used in a lot of ways, some of which are presented below.


\subsection*{Rasterized Voxel Rendering} \label{subsec-rasterized-voxel-rendering}

The basic approach to rendering voxels is using the traditional rasterized rendering pipeline.
One of the most popular games of all time, \emph{Minecraft} (Mojang \cite{Mojang2024}, 2011) uses 
this approach to render thousands of cube-shaped voxels on screen. \emph{Minecraft} (Mojang 
\cite{Mojang2024}, 2011) also makes use of various rendering optimizations to improve frame times. 
This approach is straightforward in concept and can be implemented for existing \ac{API}s, making 
use of hardware acceleration, i.e., the \ac{GPU}. The voxel representation allows for easy 
computation since geometry is laid out uniformly. Lighting calculations, as well as physics 
operations, can be further optimized to be even more efficient. The most significant characteristic 
is the ability to dynamically enable or disable voxels within the scene layout, or, in general, 
to update voxels and their corresponding data efficiently. Since voxels are uniform shapes, 
they can be easily created on the \ac{GPU} as opposed to being sent over the \ac{PCI Express} port. \\

\noindent
Due to their uniform nature, voxels are often used for real-time ray tracing. Although ray tracing 
is not considered for this work, there are still acceleration techniques and approaches for voxel 
data layouts, which are used in ray traced voxel rendering. As long as these techniques can be 
applied to rasterized voxel rendering, they are of interest for this work's approach.


\subsection*{Voxel Physics} \label{subsec-voxel-physics}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/teardown.jpg}
    \caption{The voxel based sandbox Teardown (2022) by Tuxedo Labs \cite{TuxedoLabs2022}.}
    \label{fig:teardown}
\end{figure}

\noindent
Tuxedo Labs' game \emph{Teardown} (\cite{TeardownSteam2022}, 2022) (Figure \ref{fig:teardown}) uses ray-traced 
lighting, which is affordable due to the voxel scene representation \cite{TuxedoLabs2022}. Additionally, 
it makes use of highly dynamic physics operations, which are relatively efficient using the voxel representation. 
In its base layout, a cubical voxel is an \ac{AABB}, which allows for trivial collision tests. When rotated 
around any axis, a voxel is still an \ac{OBB}, making collision tests more expensive than when using an \ac{AABB}, 
but still rather efficient. These characteristics make voxels excellent candidates for dynamic physics 
calculations. Also, storing voxels in a spatial container helps keep collision checks to a minimum. \\

\noindent
Although voxel physics is not considered for this work, it is very suitable for this approach as it basically 
allows dynamic operations like physics or simulations. The performance of the proposed concept under dynamic 
conditions needs to be tested in future experiments to see if it can support physics interactions.


\section{Culling Techniques} \label{sec-culling-techniques}

\emph{Culling} refers to the technique of discarding parts of a scene "that are not considered to contribute to 
the final image" \cite{AkenineMoeller2018}. This allows for expensive scenes to be rendered, since most parts of it 
will probably not be visible by the camera at the same time. Ignoring non-visible objects or triangles is therefore a 
common process in real-time rendering, because "the fastest triangle to render is the one never sent to the \ac{GPU}" 
\cite{AkenineMoeller2018}. In this section, common culling techniques are presented and analyzed. 


\subsection*{Backface Culling} \label{subsec-backface-culling}

Backface culling is concerned with discarding triangles that are facing away from the camera. 
These triangles will not be seen by the camera anyway and thus can be safely ignored in the rendering process. 
To compute the facing direction of a triangle, which when normalized is equivalent to the triangle's 
\emph{Surface Normal}, the vertex \emph{winding order} of the given rendering backend is considered.

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/graphics/winding-order-triangle.jpg}
    \caption{Two possible winding orders of triangles \cite{Michel2016}.}
    \label{fig:triangle-winding-order}
\end{figure}

\noindent
When describing a triangle mathematically, the vertex positions are considered, which perfectly define the edges of 
said triangle. The surface can then be interpreted as facing towards the viewer or away from them. 
When trying to compute the triangle, its vertex positions can be recorded in an arbitrary order, but to encode which 
direction the surface is facing, the vertex position order is specified up front. Microsoft's rendering \ac{API}s D3D11 
and D3D12, for example, encode a counterclockwise winding order to be interpreted as facing towards the viewer 
\cite{D3DTopology2020}. With this standard in place, all triangles can uniquely define their orientation relative to 
the camera. When enabling backface culling within the rendering backend, triangles that are not facing the camera 
are automatically omitted immediately after screenmapping \cite{AkenineMoeller2018}. Depending on the type of geometry, 
backface culling can often remove around half of the triangles about to be rendered, proving very valuable to highly 
populated scenes and high geometric density.

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/cluster-backface-culling.jpg}
    \caption{A common normal cone of a triangle cluster. It can be used to calculate a frontfacing
    and a backfacing cone. If the camera is located in either of these cones, all triangles in the 
    cluster are facing toward the camera, or away from it respectively \cite{AkenineMoeller2018}.}
    \label{fig:cluster-backface-culling}
\end{figure}

\noindent 
Backface culling algorithms can also be applied to clusters of triangles. As Shirmun et al. \cite{Shirmun1993} 
propose, multiple triangles can be culled at once considering a common normal cone, constructed by the normals 
of all the triangles within the cluster. "Shirmun and Abi-Ezzi \cite{Shirmun1993} prove that if the viewer is 
located in [a] frontfacing cone, then all faces in the cone are frontfacing, and similarly for the backfacing 
cone" \cite{AkenineMoeller2018}. As mentioned in Chapter \ref{subsubsec-meshlet-backface-culling}, this approach 
can be used for more parallelized backface culling using an updated rendering pipeline in current graphics 
\ac{API}s. A visual representation is provided in Figure \ref{fig:cluster-backface-culling}.


\subsection*{View Frustum Culling} \label{subsec-view-frustum-culling}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/view-frustum-culling.jpg}
    \caption{The camera's view frustum, relative to the scene. All objects that are completely outside of 
    the view frustum can be ignored by the rendering pipeline, since they do not contribute to the resulting image.
    The objects outside of the view frustum are colored red. Image by \cite{Pan2020}.}
    \label{fig:view-frustum-culling}
\end{figure}

\noindent
\emph{View Frustum Culling} is a technique that aims to remove instances, i.e. meshes that are outside of the camera 
view frustum. This leads to the skipping of all instances that are completely invisible due to their location relative to 
the camera. Said instances can thus be safely ignored and discarded early in order to keep the memory occupation low. 
Figure \ref{fig:view-frustum-culling} shows how the view frustum might look in relation to the scene. 
To cull potential instances, bounding volumes are calculated and checked against the camera view frustum, which in turn 
is specified by the \emph{near plane} and \emph{far plane}, as well as the \emph{fov}. \\

\noindent
In combination with acceleration structures like an octree, view frustum culling can be computed for the bounding 
volumes of the given structure, which can further optimize the culling operations due to spatial grouping of multiple 
instances. This technique is also applied in this work's implementation, using spheres as approximations for the 
cubical octree bounding volumes.


\subsection*{Occlusion Culling} \label{subsec-point-based-occlusion-culling}

When considering highly populated scenes, there might be cases of meshes occluding other meshes. This results 
in overdraw when rasterizing the final projection, which means that multiple color values will be calculated 
for the same pixel before the final color is determined. In general, overdraw should be avoided where possible. 
Having a lot of overdraw within the rendering process means that visibility determination of instances is deferred 
to the last possible point in the rendering pipeline. Often, it is better to pre-determine visibility and sample 
only visible instances. To achieve this, algorithms can be used to select and omit occluded instances. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/bounding-volume-quality.jpg}
    \caption{Different bounding volumes provide different advantages and disadvantages \cite{Six2021}.}
    \label{fig:bounding-volumes}
\end{figure}

\noindent
Different occlusion culling algorithms can be applied depending on scene layout, object characteristics, and other 
factors. Algorithms have been proposed that operate in \emph{image space}, \emph{object space}, or \emph{ray space}, 
with image space occlusion culling being most widely adopted in real-time rendering \cite{AkenineMoeller2018}. 
These algorithms operate on a projected 2D image to consider the visibility of different instances. As is the case 
with culling algorithms in general, occlusion culling algorithms usually rely on bounding volumes to approximate 
object positions and scale in space. Figure \ref{fig:bounding-volumes} shows how different bounding volumes can be 
used to support faster computation or more accurate volume approximation.


\paragraph*{Hierarchical Z-Buffering} \label{subsubsec-hierarchical-z-buffering}

\emph{\ac{HiZ} Buffering} is a technique first introduced for \ac{CPU} computation by Ned Greene et al. 
\cite{Greene93,Greene95}. Since then, it "has had significant influence on occlusion culling research" 
\cite{AkenineMoeller2018} and plays a central role in this work. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/hiz-mip-chain.jpg}
    \caption{A mip chain of a scene's z-buffer. Every buffer in this chain is a quarter of the resolution of its 
    predecessor buffer \cite{Schachtschabel2017}.}
    \label{fig:hiz-mip-chain}
\end{figure}

\noindent
\ac{HZB} is computed in image space and relies on two main components. An octree scene representation and a 
\emph{z-buffer} pyramid. The octree contains all instances present in the scene. The \ac{HiZ} pyramid is a set of 
z-buffer mip-maps, with every map reducing in size relative to the previous mip-map. It is empty in the beginning 
and is gradually filled after the depth buffer is populated. Each mip level of the \ac{HiZ} pyramid is computed by 
aggregating values from 4 texels (texture pixels) within the previous mip-map and writing the output value into one 
texel of the new mip level. This way, both axes (u and v) are scaled to be half the length of the previous level. 
The resulting texture resource then has the full-resolution z-buffer as the highest-resolution mip-map and a very 
low-resolution and small mip-map on the other end of the mip chain. An exemplary \ac{HiZ} pyramid is shown in figure 
\ref{fig:hiz-mip-chain}.\\

\noindent
In contrast to the classic generation of mip-maps, the computation of the values in the \ac{HiZ} pyramid works in a 
very specific way. "[...] [E]ach z-value is the farthest z in the corresponding 2 \begin{math}\times\end{math} 2 window 
of the adjacent finer level" \cite{AkenineMoeller2018}. This process is shown in Figure \ref{fig:hiz-value-computation}, 
visualizing how the farthest value of each texel window is inserted into adjacent levels.\\

\begin{figure}[!]
    \centering
    \includegraphics[width=200px]{images/graphics/hiz-buf-values.jpg}
    \caption{The calculation of a \ac{HiZ} pyramid computes the farthest value of each texel window, 
    resulting in a new texel value for adjacent mip levels \cite{AkenineMoeller2018}.}
    \label{fig:hiz-value-computation}
\end{figure}

\noindent
A visibility test against this \ac{HiZ} pyramid is computed as follows: The octree is traversed, and each octree node 
is transformed into image space, essentially projecting the bounding volume of the node onto the view plane. Now, 
the projected bounding volume overlaps a specific region of the view plane, which is then tested against the lowest 
resolution \ac{HiZ} pyramid texel group, containing this volume projection. The nearest z value of the bounding volume is 
compared to the texel group's farthest value entry. If the bounding volume's z happens to be farther than the 
texel group value, it is consequently occluded. If the comparison yields the opposite result, the test "is continued 
recursively down the \ac{HiZ} pyramid until the [bounding volume] is found to be occluded, or until the bottom level of the 
\ac{HiZ} pyramid is reached, at which point the box is known to be visible" \cite{AkenineMoeller2018}. Visible octree nodes 
are then drawn to the depth buffer, meaning all the geometry contained within the node is drawn to the depth buffer 
for testing against during the computation of the following octree nodes.\\

\noindent
This works because the \ac{HiZ} pyramid encodes the closest object's depth for each pixel, and, by hierarchically 
aggregating the depth values, approximating the depth for larger and larger groups of pixels. Checking bounding boxes 
against the lowest resolution mip-map in the \ac{HiZ} pyramid requires fewer samples than checking the same bounding 
box against a higher resolution mip-map. Consequently, objects that are relatively far behind a potential occluder 
can be determined very efficiently. \\

\noindent 
As Akenine-Möller et al. \cite{AkenineMoeller2018} point out, this algorithm is not used in its original form. 
Modern variations of this algorithm evolved from the work of Greene et al. \cite{Greene93} and are widely adopted in 
modern real-time computer graphics. As will be shown next, the concept of an octree representation used for 
hierarchical culling and a \ac{HiZ} pyramid used as an occlusion representation (\cite{AkenineMoeller2018}) can be elegantly 
leveraged on the \ac{GPU} side for efficient occlusion culling.


\paragraph*{Two-Pass Occlusion Culling} \label{subsubsec-two-pass-occlusion-culling}

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/graphics/depth-buffer-ac-unity.jpg}
    \includegraphics[width=200px]{images/graphics/final-frame-ac-unity.jpg}
    \caption{The depth buffer (left) and the final back buffer (right) of \emph{Assassin's Creed Unity} (\emph{Ubisoft}, 2014 \cite{Ubisoft2014}). 
    The hand-picked \emph{best occluders} were already drawn to the depth buffer in a depth pre-pass.
    While rendering all the geometry to the back buffer, each instance can be checked against the \ac{HiZ} pyramid \cite{Kruskonja2022}.}
    \label{fig:depth-buffer-ac-unity}
\end{figure}

\noindent
\emph{\ac{TPOC}} is an adaptation of the \ac{HZB} algorithm. One of the most renowned examples of this algorithm is 
present in the 2014 title \emph{Assassin's Creed Unity} by \emph{Ubisoft} \cite{Ubisoft2014}. One year later, Ulrich 
Haar and Sebastian Aaltonen presented their approach at SIGGRAPH 2015 \cite{Aaltonen2015}. Their implementation 
inspired other developers, like \emph{Epic Games} for \emph{Unreal Engine 5}, to adopt similar techniques \cite{Karis2021}. \\

\noindent
Some use cases are especially well suited for the application of occlusion culling. \emph{Assassin's Creed Unity}, for 
example, has a large open world in a city with many large buildings. A rather large amount of instances can therefore be 
expected to be occluded by some of the buildings at any given moment. \emph{Assassin's Creed Unity's} \ac{TPOC} 
implementation uses artist-picked \emph{best occluders} for \ac{HZB} computation. During runtime, the best occluders are 
drawn to the depth buffer in a dedicated depth pre-pass and then used as described in the next section. 
The output of the depth buffer pre-pass is shown in Figure \ref{fig:depth-buffer-ac-unity}. Depth passes can be done 
quite efficiently, especially if they only contain a handful of instances to draw. Additionally, because of the advances 
in \ac{GPU}-driven rendering, the \ac{HiZ} pyramid can be computed by a \emph{compute shader}, which leverages the 
highly parallel computation capabilities of the \ac{GPU}. \\

\noindent
In contrast to the basic \ac{HZB}, this approach relies on the heavy use of \ac{GPU} computation and the 
pre-determination of the best occluders. This way, the scene doesn't have to be reversed in a front-to-back order 
and does not necessarily need to incorporate an octree, as long as object bounds can be computed efficiently enough.
Aaltonen et al. \cite{Aaltonen2015} also mention the addition of depth reprojection, which is compatible with 
\ac{TPOC}, to further optimize the occlusion culling technique.


\paragraph*{Depth Reprojection} \label{subsubsec-depth-reprojection}

\emph{Depth reprojection} makes use of the temporal coherence of the z-buffer. The assumption is made that 
between two consecutive frames, most of the visible content remains the same. It is obvious that this will 
hold true for the most part when the camera's rotation and position aren't changed in between frames. In 
this case, the last frame's depth, color, and visibility information would remain nearly identical. The only 
differences would be introduced by animation or physics simulation, which could move instances independently 
from the camera movement. But even with the camera moving over frames, a large part of the scene - static 
parts like buildings or terrain in particular - would retain a lot of its information. This means that there 
will be a significant coherence in visibility between frames, which depth reprojection makes use of.\\

\noindent
The algorithm stores the last frame's depth information and "reprojects" it onto the new frame. In the best case, 
most instances that were previously visible remain visible in this new frame, and instances that were invisible 
in the previous frame remain invisible in the new frame as well. The new camera transformations and a 
\emph{velocity vector buffer} are used to calculate the delta relative to the previous frame and create an 
approximation of the new depth buffer. Because the reprojected depth buffer is only an approximation, visibility 
and depth calculations can be imprecise because the result is non-conservative. This means that the reprojection 
process sometimes fails to compute the correct depth values for objects, especially if they move relative to the 
camera. Another problem occurs in use cases where the camera moves very fast relative to the environment. In such 
a case, the camera might cover too large a distance so that none of the visible instances in a given frame were 
visible during the previous frame \cite{Kruskonja2022}. \\


\section{Mesh Shading}  \label{sec-mesh-shading}

Mesh shaders were first introduced to NVIDIA Turing \ac{GPU}s in 2018 as a part of "a new programmable 
geometric shading pipeline" and built upon the compute programming pipeline \cite{Kubisch2018}. 
Therefore, they aim to optimize work by using the available hardware more efficiently. Compared to the 
traditional vertex shading pipeline, mesh shading introduces a more holistic and parallel processing of 
geometry data. It is incorporated into the rasterized rendering pipeline and has since been used in modern 
gaming. It gained a lot of attention when it was used as the foundation of Epic Games' \emph{Unreal Engine 5} 
feature \emph{Nanite} \cite{Karis2021}.\\


\subsection*{Mesh Shading Pipeline} \label{subsec-the-mesh-shading-pipeline}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/mesh-rendering-pipeline.jpg}
    \caption{The mesh shading pipeline as presented by NVIDIA in \cite{Kubisch2018} (modified).}
    \label{fig:mesh-rendering-pipeline}
\end{figure}

\noindent
The \emph{mesh shading rendering pipeline} is a rendering pipeline optimized for scenes with complicated 
geometry. It shares the rasterizer and the pixel shader stages with the vertex shading pipeline but 
introduces two new stages, which aim to replace the vertex shader, geometry shader, and tesselation stage.
The new stages are called \emph{task shader} (or \emph{amplification shader}) and \emph{mesh shader}. Both 
are fully programmable and are based on the compute shaders architecture. Therefore, they are inherently 
different from the traditional vertex shader stage, even though they both can be used for vertex 
transformations. Figure \ref{fig:mesh-rendering-pipeline} shows the pipeline stages and the order they are 
executed in. \\

\noindent
The task shader serves as a precomputational stage, which is optional and invokes the mesh shader. It can 
take any arbitrary data as input and can operate on the provided data to produce output data, which is then 
scheduled for further computation. Since both shaders are based on compute shaders, a thread group size can 
be specified, and group-shared memory can be allocated. This allows for highly parallelized execution and 
optimized memory usage. A key difference to the traditional pipeline is that the task shader can invoke other 
(compute) shaders, create new tasks, and dispatch none, one, or multiple mesh shader invocations. This makes 
the shader extremely flexible and capable of providing the same results as the geometry shader does, but 
faster and more scalable. It is often used for early culling, for efficient tessellation, or for \ac{LOD} 
decisions \cite{Kubisch2018}. The invocation of the next stage is called using the \emph{DispatchMesh()} 
function, which invokes the mesh shader. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/bunny-meshlet.jpg}
    \caption{The \emph{Stanford Bunny} mesh geometry split into meshlets. Each meshlet is represented by a single color, 
    and consists of multiple triangles \cite{Oberberger2024}.}
    \label{fig:bunny-meshlet}
\end{figure}

\noindent
The mesh shader output can be defined by the user and is usually configured to be a small patch consisting of a small 
number of triangles, called a \emph{meshlet}, which is shown in Figure \ref{fig:bunny-meshlet}. This new approach 
to operating on vertices allows for a new interpretation of mesh data. The primitive topology is not anymore constrained 
to triangles, triangle fans, and triangle strips but can be individually specified by the developers.
Meshlets are computed by thread groups, which usually fetch the mesh data from the given vertex and index buffers and 
apply transformations and arbitrary operations. This new way of processing geometry is not only more efficient in many 
cases and reduces the memory bandwidth, but also allows for per-meshlet operations \cite{Kubisch2020}. This is a key 
advantage, since the geometry of a complete mesh can now be processed in parallel and still be individually discarded 
or post-processed. There are several other advantages over the traditional vertex shader pipeline, which will not be 
listed to the full degree. More details are provided in the blog post by NVIDIA \cite{Kubisch2020}.\\


\subsection*{Meshlet Culling} \label{subsec-meshlet-culling}

One major innovation that is enabled by the use of mesh shading is to cull on a per-primitive (per-meshlet) basis. 
This allows for more granular culling of high-density geometry even within whole meshes. Prior to the implementation 
of meshlets, culling usually was done on a per-mesh (also referred to as \emph{per-instance}) basis (or for backface 
culling on a per-triangle basis), omitting whole meshes depending on their position in space or their visibility from 
the point of view of the camera.\\

\noindent
Although the mesh shading pipeline was first introduced in 2018, the idea of triangle clusters goes back even further.
Aaltonen et al. \cite{Aaltonen2015} provide an overview of multiple cluster-related optimization techniques, implementing 
a highly parallelized rendering pipeline before mesh shading became part of the official rendering \ac{API}s.
For their \ac{GPU} culling algorithms, they refer to the prior work of Hill \cite{Hill11} and Greene \cite{Greene93}, which 
provides the basis for some of the implementation in this work's approach. Most of the culling algorithms discussed in chapter 
\ref{sec-culling-techniques} can be adopted to use meshlet culling. The algorithms can usually be applied to triangle clusters 
without the need to change a lot of the algorithm. Bounding volumes can be calculated for meshlets, too, and meshlets do also 
consist of triangles themselves. In the following, the basic culling algorithms are briefly revisited in order to show how they 
can be adapted for the mesh shading pipeline.


\paragraph*{Meshlet View Frustum Culling} \label{subsubsec-meshlet-view-frustum-culling}

Frustum culling can be adapted to work on meshlets as well. The basic frustum culling technique remains the same but 
is applied to triangle clusters (meshlets) rather than instances (meshes). To implement this, meshlet bounding volumes 
need to be computed. The same principles apply to meshlet bounding volumes as to instance bounding volumes, so spherical 
volumes are usually fast and cheap to calculate. If the meshlets are pre-calculated, the bounding volumes can be 
precalculated as well, since they will not change over time. Then, frustum culling is as simple as using any existing 
frustum culling algorithm (see Chapter \ref{subsec-view-frustum-culling}) and applying it to the meshlets. \\

\noindent
Since this can be done in the task shader, the culling can be computed in parallel, and culled meshlets can 
be discarded early. This includes parts of single instances, which allows for even better optimization. In 
the traditional vertex shading pipeline, instances had to be completely outside of the view frustum in order 
to be culled. If an instance is partially visible in the mesh shading pipeline, the meshlets inside the frustum 
can be drawn, and the meshlets outside of the frustum can be discarded.


\paragraph*{Meshlet Occlusion Culling} \label{subsubsec-meshlet-occlusion-culling}

Meshlet occlusion culling refers to the process of culling meshlets, which are occluded by any other meshlet, a 
group of meshlets, or another instance. It can be implemented in various ways, analogous to similar \ac{GPU}-based 
occlusion culling algorithms. Often, this implies the use of a \ac{BVH} like an octree to make visibility checks 
easier. \ac{HZB} can also be applied to the new pipeline, and meshlets can be checked for occlusion using the 
highly parallel mesh shading pipeline. Consequently, \ac{TPOC} is also applied to individual meshlets, which was 
implemented in Alan Wake 2 (Remedy Entertainment \cite{Remedy2023}, 2023) or \emph{Epic Games'} \emph{Unreal Engine 5} and 
\emph{Nanite} \cite{Karis2021}. \\

\noindent
It is assumed that this new way of culling single meshes can generate new innovation in the field of computer graphics.
This is why the proposed approach is implemented using the mesh shading pipeline, which, when used carefully, is more 
efficient than comparable methods like the use of geometry shaders or instancing.