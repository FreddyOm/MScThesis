\chapter{Technical Background} \label{cpt-technical-background}

\section{Voxel Scene Layout}

The voxel rendering in our evaluation relies on a volumetric representation of the scene.
It is particularly important that the voxel model not only includes the surface but also 
the space within the model. This constraint is often used in interactable use cases, where 
the model is split, cut or manipulated in any given way. [@TODO: Reference Minecraft] lets
the player take full control over the sandbox environment and allows adding or subtracting 
voxels as they wish. Essentially, the voxel data of every possible voxel within the playable 
space needs to be somehow present or encoded in memory, even though just a fraction of the 
voxels are actually drawn to the screen.\\

\subsection{Three Dimensional Grid} \label{subsec-three-dimensional-grid}

[@TODO: Additional option of drawing bitwise]
A basic approach for a scene representation is a three dimensional voxel grid. This 
approach relies on a fixed size grid where each grid cell represents one voxel.
To draw the voxels, a seperate buffer can hold additional voxel information, which 
can be accessed by a given grid cell index. The voxel data can store information about 
whether a voxel is present in that particular grid cell or not, which color the voxel 
should have, the normals for lighting calculation and any other data necessary.
This approach is relatively lightweight but inefficent for large grid sizes and low 
grid occupations. All grid cells need to be traversed to draw any given scene which 
might include a lot of empty grid cells. Additionally, storing the voxel data seperately
introduces a pointer indirection on each step of the traversal, potentially leading to more 
cache misses.

\subsection{Octree Data Structure} \label{subsec-octree-data-structure}

[@TODO: source] proposes the use of a spatial container to optimize traversal. The use of an octree 
incorporates the relevant scene space and subdivides it into smaller child nodes. When inserting data 
into the octree, the position is evaluated and the data is being stored as a \emph{payload} in 
a node which includes the given position. If a node holds more payload instances than a specified threshold 
allows, it is split into eight child nodes and the payload originally present in the node is 
distributed onto the child nodes. This process is repeated until all data is finally inserted 
into the octree. The resulting octree maintains all the relevant payload data, having a deeper tree 
hierarchy where more data is present and a shallow hierarchy where almost no or no data is present.

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/octree.png}
    \caption{An octree data structure. It stores data alongside its spatial characteristics, relative to 
    other data entries \cite{Six2021}.}
    \label{fig:octree}
\end{figure}

[@TODO: Mention other spatial containers -> Binary Space Partitioning Trees]

Octrees like this can vary in their specific implementation, especially in how they maintain the 
payload. Some might store it right next to the octree node description, others might want to separate 
octree and payload data. Octree nodes could also be appended to a dedicated buffer and therefore be stored 
seperately to the tree hierarchy description. Such an approach would maintain chache coherency during an 
unordered traversal of all octree nodes. [@TODO: find appropriate sources]

When using hierarchical data structures, dedicated vertex positions can be omitted and instead 
the voxel position in space can be implicitly calculated from the hierarcy and the root nodes bounds. 
This allows for improvement of memory occupation, which is vital for loading and storing high 
resolution voxel scenes. A voxels state can be encoded into either visible (1) or empty (0).
A bitwise representation of voxels can significantly decrease the memory footprint which is a concept 
on which the next approach expand on.

\subsection{High Resolution Sparse Voxel Directed Acyclic Graphs} \label{subsec-highres-svo-dags}

Assarsson et al. \cite{Assarsson2013} use data redundancy to further generalize and optimize high resolution 
\ac{SVO}s. In their implementation the octree data structure relies on bitmasks, to encode the existance of child 
nodes. This way, empty nodes are ignored and the information can be easily stored within one byte. This child node 
mask uniquely identifies the layout of the nodes below and can also be used to store the voxel occupation of the 
leaf nodes. Next, they make use of the fact, that an octree nodes position is uniquely identifed by the path along 
the nodes from root to leaf node. Since voxels can be encoded as either being present or not, and non-present 
voxels can be ignored, the data can be significantly reduced in size. Furthermore, equal child node masks 
can simply be shared between different parent nodes instead of being duplicated. Assarsson et al. \cite{Assarsson2013} 
suggest a buttom-up approach to remove redundant nodes and replace whole sub-trees of the octree by pointing to one 
instance of the same pattern within the tree, which is shown in figure \ref{fig:sparse-voxel-dag-creation}. 
This optimization can be recursively done to each layer in the tree hierarchy, resulting in an optimized version 
of the data structure. The resulting structure is a more general hierarchical data structure, namely a \ac{DAG}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/highres-sv-dag.png}
    \caption{Reducing a \ac{SVO} using the approach of \cite{Assarsson2013}.}
    \label{fig:sparse-voxel-dag-creation}
\end{figure}

\noindent
This approach seems to be the best solution for reading, storing and optimizing octree data, although we use 
a more basic approach in our implementation. Nevertheless, this concept of optimizing voxel data is compatible 
with our use case and is highly recommended by us.

\section{Culling Techniques} \label{sec-culling-techniques}

\emph{Culling} refers to the technique to discard parts of a scene "that are not considered to contribute to the final 
image" \cite{AkenineMoeller2018}. This allows for a highly populated scenes to be rendered, since most parts of a scene 
will probably not be visible by the camera at the same time. Ignoring culled objects or triangles is therefore a common 
process in realtime rendering. "The fastest triangle to render is the one never sent to the \ac{GPU}" 
\cite{AkenineMoeller2018}. In this section, the most common culling techniques are presented and the ones considered by 
our approach are specifically highlighted and analyzed. 


\subsection{Backface Culling} \label{subsec-backface-culling}

Backface culling is probably the most commonly used culling algorithm. It is concerned with discarding triangles which 
are facing away from the camera. These triangles will not be seen by the camera anyway and thus can be safely ignored 
in the rendering process. To compute the facing direction of a triangle, which when normalized is equivilent to the 
triangles \emph{Surface Normal}, the vertex winding order of the given rendering backend is considered.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/graphics/winding-order-triangle.png}
    \caption{Two possible winding orders of triangles \cite{Michel2016}.}
    \label{fig:triangle-winding-order}
\end{figure}

\noindent
When describing a triangle mathematically, the vertex positions are considered, which perfectly define the edges of 
said triangle. The surface can then be interpreted as facing towards the viewer or away from them. 
When trying to compute the triangle, its vertex positions can be recorded in an arbitrary order, but to encode which 
direction the surface is facing, the vertex position order is specified up front. Microsoft's rendering \ac{API}s D3D11 
and D3D12 for example encode a counterclock-wise \emph{winding order} to be interpreted as facing towards the viewer 
\cite{D3DTopology2020}. With this standard in place, all triangles can be uniquely define their orientation relative 
to the camera. When enabling backface culling within the renderin backend, triangles which are not facing the camera, 
are automatically omitted immediatly after screenmapping \cite{AkenineMoeller2018}. Depending on the type of geometry, 
backface culling can remove up to half of the triangles about to be rendered, proving very valuable to highly populated 
scenes and high geometric density.\\

\noindent 
[@TODO: Add image]
Backface culling algorithms can also be applied to clusters of triangles. As \cite{Shirmun1993} propose, multiple 
triangles can be culled at once considering a common normal cone. "Shirmun and Abi-Ezzi \cite{Shirmun1993} prove that 
if the viewer is located in [a] frontfacing cone" constructed by the normals of all the triangles within the cluster, 
"then all faces in the cone are frontfacing, and similarly for the backfacing cone" \cite{AkenineMoeller2018}. As 
will be mentioned later in chapter \ref{subsubsec-meshlet-backface-culling}, this approach can be used for more 
parallelized backface culling using an updated rendering pipeline in current graphics \ac{API}s.

\subsection{View Frustum Culling} \label{subsec-view-frustum-culling}

[@TODO: Add pictures]

\emph{View Frustum Culling} is a technique which aims to remove instances, i.e. meshes, which are outside of the camera 
view frustum. This leads to a discard of all instances which are completely invisible due to their location relative to 
the camera. Said instances can thus be safely ignored and discarded early in order to keep the memory computations by 
\ac{CPU} and \ac{GPU} low. To cull potantial instances, bounding boxes are calculated and checked against the camera 
view frustum, which in turn is specified by the \emph{near plane} and \emph{far plane}, as well as the \emph{fov}. For 
instance, if a bounding sphere is partially or completely inside the view frustum can then be tested by computing a 
simple \boldmath{dot product} between the frustum plane and the bounding volume center and comparing it against the 
radius of the bounding sphere. \\

\noindent
In combination with acceleration structures like an octree, view frustum culling can be computed for the bounding 
volumes of the given structure which can further optimize the culling operations due to spatial grouping of multiple 
instances. This technique is also applied in our approach, using spheres as approximations for the cubical octree 
bounding volumes.

\subsection{Occlusion Culling} \label{subsec-point-based-occlusion-culling}

When considering highly populated scenes, there might be cases of meshes occluding other meshes. This produces overdraw 
when rasterizing the final projection, which means that multiple color values will be calculated for the same pixel 
before the final color is determined. In general, overdraw should be avoided where possible. Having a lot overdraw 
within the rendering process means that visibility determination of instances is deferred to the last possible point in 
rendering pipeline. Often, it is better to pre-determine visibility and sample only visible instances. To achieve this, 
algorithms can be used to select and ommit occluded instances. \\

\begin{figure}[h]
    \centering
    \includegraphics[width=300px]{images/graphics/bounding-volume-quality.png}
    \caption{Different bounding volumes provide different advantages and disadvantages \cite{Six2021}.}
    \label{fig:bounding-volumes}
\end{figure}

\noindent
Different occlusion culling algorithms can be applied depending on scene layout, object characteristics and other factors.
As it is the case with culling algorithms in general, occlusion culling algorithms usually rely on bounding volumes to 
approximate object positions and scale in space. Figure \ref{fig:bounding-volumes} shows how different bounding volumes 
can be used to support faster computation or more accurate volume approximation. Algorithms have been proposed which 
operate in \emph{image space}, \emph{object space} or \emph{ray space}, with image space occlusion culling being most 
widely adopted in realtime rendering \cite{AkenineMoeller2018}. These algorithms operate on a projected 2D image to 
consider visibility of different instances.  

\subsection{Hierarchical Z-Buffering} \label{subsec-hierarchical-z-buffering}

\emph{\ac{HiZ} Buffering} is a technique first introduced for \ac{CPU} computation by Ned Greene et al. 
\cite{Greene93,Greene95}. Since then, it "has had significant influence on occlusion culling research" 
\cite{AkenineMoeller2018}. It is a fundamental basis on which our approach relies on which will be discussed in more detail.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/graphics/hiz-buf-values.png}
    \caption{The calculation of a \ac{HiZ} buffer computes the farthest value of each texel window, 
    resulting in a new texel value for adjacent mip levels \cite{AkenineMoeller2018}.}
    \label{fig:hiz-value-computation}
\end{figure}

\noindent
\ac{HZB} is computed in image space and relies on two main components. An octree scene representation and a 
\emph{Z-Buffer} pyramid. [@TODO: Check if z-buffer or mip map needs to explained] 
The octree contains all instances present in the scene. The \emph{z-pyramid} is a set of z-buffer mip-maps, with every 
map reducing in size relative to the previous mip map. The reduction is computed by aggregating values from 4 texels 
within the previous mip-map and writing the output value into one texel of the new mip-map level. This way, both axis 
(u and v) are scaled to be half the length of the previous level. The resulting z-pyramid then has the full resolution 
z-buffer as the finest granularity in the z-pyramid and a very low resolution and small texture on the other end. 
An exemplary z-pyramid is shown in figure \ref{fig:hiz-mip-chain}.\\

\noindent
The computation of new values is another crucial step. "[...] [E]ach z-value is the farthest z in the corresponding 
2 \begin{math}\times\end{math} 2 window of the adjacent finer level" \cite{AkenineMoeller2018}. This process is shown 
in figure \ref{fig:hiz-value-computation}, showing how the farthest value of each texel window is inserted into adjacent 
levels.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/hiz-mip-chain.png}
    \caption{A mip chain of a scenes z-buffer. Every buffer in this chain is has quater of the resolution of its 
    predecessor buffer \cite{Schachtschabel2017}.}
    \label{fig:hiz-mip-chain}
\end{figure}

\noindent
A visibility test against this z-pyramid is computed as follows. The octree is traversed in a front to back order 
and each octree node is transformed into image-space, essentially projecting the bounding volume of the node onto 
the view plane. Now, the projected bounding volume overlaps a specific region of the view plane, which is then 
tested against the coarsted z-pyramid cell, containing this volume projection. The nearest z value of the bounding 
volume is compared to the z-pyramids farthest value entries. If the bounding volumes z happens to be farther than the 
\ac{HiZ} value, it is consequently occluded. If the comparison yields the opposite result, the test "is continued 
recursively down the z-pyramid until the [bounding volume] is found to be occluded, or until the bottom level of the 
z-pyramid is reached, at which point the box is known to be visible" \cite{AkenineMoeller2018}. Visibile octree nodes 
are then drawn to the depth buffer, meaning all the geometry contained within the node is drawn to the depth buffer 
for being tested against during the computation of the following octree nodes. As Akenine-Möller et al. 
\cite{AkenineMoeller2018} point out, this algorithm is not used in its original form. Modern variations of this 
algorithm evolved from the work of Greene et al. \cite{Greene93} and are widely adopted in modern real-time computer 
graphics. As will be shown in chapter \ref{subsec-two-pass-occlusion-culling}, the concept of an octree representation 
used for hierarchical culling and a z-pyramid used as an occlusion representation (\cite{AkenineMoeller2018}) can be 
elegantly leveraged on the \ac{GPU} side for efficient occlusion culling.


\subsection{Two-Pass Occlusion Culling} \label{subsec-two-pass-occlusion-culling}

\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{images/graphics/depth-buffer-ac-unity.png}
    \includegraphics[width=200px]{images/graphics/final-frame-ac-unity.png}
    \caption{The depth buffer (left) and the final back buffer (right) of \emph{Assassin's Creed Unity} (Ubisoft, 2014 \cite{Ubisoft2014}). 
    The hand-picked \emph{best occluders} were already drawn to the depth buffer in a depth pre-pass.
    While rendering all the geometry to the back buffer, each instance can be checked against the z-pyramid \cite{Kruskonja2022}.}
    \label{fig:depth-buffer-ac-unity}
\end{figure}

\noindent
\emph{Two-Pass Occlusion Culling} is an adaptation of the \ac{HZB} algorithm, introduced in chapter 
\ref{subsec-hierarchical-z-buffering}. One of the most reknown examples of this algorithm is present in the 2014 
title \emph{Assassin's Creed Unity} by Ubisoft \cite{Ubisoft2014}. One year later, Ulrich Haar and Sebastian Aaltonen 
presented their approach on SIGGRAPH 2015 \cite{Aaltonen2015}. Their implementation inspired other developers like 
\emph{Epic Games} for \emph{Unreal Engine 5} to adopt similar techniques \cite{Karis2021}. This is the algorithm we 
use in our implementation. It shares its basic principle with \ac{HZB} but focuses on a more \ac{GPU}-Driven 
configuration and using a given set of best occluders to cull instances effectively. \\

\noindent
Some use cases are especially well suited for \ac{TPOC}, \emph{Assassin's Creed Unity} for example has a large open 
world in a city with many large buildings. A rather large amount of instances can therefore be expected to be occluded 
by some of the buildings at any given moment. \ac{TPOC} thus uses artist-picked \emph{best occluders} for \ac{HZB} 
computation. The best occluders are identified during runtime, drawn to depth buffer during a depth pre-pass, and 
then used as described in chapter \ref{subsec-hierarchical-z-buffering}. The output of the depth buffer pre-pass is 
shown in figure \ref{fig:depth-buffer-ac-unity}.Depth passes can be done quite efficiently, especially if they 
only contain a handful of instances to draw. Additionally, because of the advances in \ac{GPU}-Driven Renderin, the 
\ac{HiZ} pyramid can be computed by a \emph{compute shader}, which leverages the highly parallel computation 
capabilities of the \ac{GPU}. \\

\noindent
In contrast to the basic \ac{HZB}, this approach relies on the heavy use of \ac{GPU} computation and the pre-
determination of best occluders. This way, the scene doesn't have to be reversed in a front to back order and 
not necessarily needs to incorporate an octree, as long as object bounds can be computed efficiently enough.
In SIGGRAPH 2015 presentation, Aaltonen et al. mention the addition of another acceleration technique, which 
is compatible with \ac{TPOC}, \emph{Depth Reprojection}. 


\subsection{Depth Reprojection} \label{subsec-depth-reprojection}

\emph{Depth Reprojection} makes use of the temporal coherence of the z-buffer. The assumption is made, that 
between two consecutive frames most of the visible content remains the same. It is obvious that this will 
hold true for the most part when the cameras rotation and position isn't changed inbetween frames. In this case, 
the last frames depth, color and visibilty information would remain nearly identical. The only differences would 
be introduced by animation or physics simulation, which could move instances independently from the camera movement.
But even with the camera moving over frames, a large part of the scene - static parts like buildings or terrain in 
particular - would retain a lot of its information. This means, that there will be a significant coherence in 
visibility between frames, which Depth Reprojection makes use of.\\

\noindent
The algorithm stores the last frames depth information and "reprojects" it onto the new frame. In the best case, 
most instances which were previously visible remain visible in this new frame, and instances which were invisible 
in the previous frame remain invisible in the new frame, as well. The new camera transformations and a 
\emph{Velocity Vector Buffer} are used to calculate the delta relative to the previous frame and create an 
approximation of the new depth buffer. Because the reprojected depth buffer is only an approximation, visibility 
and depth calculations can be inprecise, because the result is non-conservative. This means that the reprojection 
process sometimes fails to compute the correct depth values for objects, especially if they move relative to the 
camera. Another problem occurs in use cases, where the camera moves very fast relative to the environment. In such 
a case, the camera might cover a too large distance, so that none of the visible instances in a given frame were 
visible during the previous frame. \cite{Kruskonja2022} \\

\noindent
The \ac{TPOC} algorithm is used in our approach, with minimal changes to the selcetion of best occluders.
Depth Reprojection could in general be used as well, but is not within the scope of our implementation.

\section{Mesh Shading}  \label{sec-mesh-shading}

Mesh Shaders were first introduced to NVIDIA Touring \ac{GPU}s in 2018 as a part of
"a new programmable geometric shading pipeline" and build upon the compute programming 
pipeline \cite{Kubisch2018}. Therefore, they aim to optimize work by 
using the available hardware more efficiently. Compared to the traditional vertex shading 
pipeline, mesh shading introduces a more holistic and parallel processing of geometry data.
It is incorporated into the rasterized rendering pipeline and has since been used in modern 
gaming. It gained a lot of attention when it was used as the foundation of Epic Games 
Unreal Engine 5 feature \emph{Nanite} [@TODO: source].\\

\subsection{The Mesh Shading Pipeline} \label{subsec-the-mesh-shading-pipeline}

In contrast to the \emph{Mesh Shading Rendering Pipeline}, the "traditional" \emph{Vertex Shading Pipeline} 
has more specialized stages. Figure \ref{fig:traditional-rendering-pipeline} shows all the stages available 
to developers. This pipeline is composed of fixed, programmable and optional stages, where each 
stage operates on some input data and in some cases produces output data for the next stage to consume.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/traditional-rendering-pipeline.png}
    \caption{The traditional rendering pipeline as presented by NVIDIA in \cite{Kubisch2018}.}
    \label{fig:traditional-rendering-pipeline}
\end{figure}

\noindent
The \emph{Vertex Attribute Fetch} stage (also called \emph{Input Assembler} stage) collects the geometric 
input and creates primitives out of it, reading all necessary data associated with the primitive data. 
These primitives are usually triangles but this can vary slightly from usecase to usecase, depending
on the prior configuration of the stage. After the Input Assembler, the data is sent to the \emph{Vertex Shader}.
This stage is responsible for operating on vertex transformations, moving, rotating, scaling and distorting the 
vertices according to the given transformation matrices. After that, the \emph{Tesselation Control Shader} can 
optionally increase geometric resolution or triangulate polygonal geometry by applying tesselation functions.
The next stage is the optional \emph{Geometry Shader}, which is capable of generating additional primitives 
on the \ac{GPU}. However, it is commonly said to be not very efficient and is treated as deprecated since the 
introduction of the more general purpose and faster \emph{Compute Shaders} [@TODO: source]. 
The three-dimensional scene is then processed by the \emph{Rasterizer} which breaks down the scene description into 
a discrete, two-dimensional "image". This final image, also called \emph{Frame Buffer}, contains the two-dimensional 
projection of the scene from the cameras point of view. This rasterization process also implies creating discrete 
samples from continuous lines and faces.
Finally, the final frame buffer is colored correctly by the \emph{Pixel Shader}. This stage takes all given parameters 
like light position, light color, light intensity, camera position, vertex colors, textures and many more into account 
and creates a final image. The pixel shader can implement different functions (\ac{BRDF}) for calculation of the final 
pixel color and is called once per pixel. This means, that the computational cost increases with the amount of pixel, 
i.e. the frame buffers resolution. 


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/graphics/mesh-rendering-pipeline.png}
    \caption{The mesh shading pipeline as presented by NVIDIA in \cite[Christoph Kubisch]{Kubisch2018}.}
    \label{fig:mesh-rendering-pipeline}
\end{figure}

The \emph{Mesh Shading Rendering Pipeline} is the a rendering pipeline, optimized for high geometrical 
density. It shares the Rasterizer and the Pixel Shader stages with the Vertex Shading Pipeline but 
introduces two new stages which aim to replace the Vertex Shader, Geometry Shader and Tesselation stage.
The new stages are called \emph{Task Shader} (or \emph{Amplification Shader}) and \emph{Mesh Shader}. Both 
are fully programmable and are based on the Compute Shader architecture. Therefore they are inherently different 
from the traditional Vertex Shader stage, even though they both can be used for vertex transformations.
Figure \ref{fig:mesh-rendering-pipeline} shows the piepline stages and the order they are executed in. \\

\noindent
The Task Shader serves as a precomputational stage which is optional and invokes the Mesh Shader.
The Task Shader can take any arbitrary data as input and operates on the data to produce output data which in 
turn is scheduled for execution on the Mesh Shader. Since both shaders are based on Compute Shaders, a thread 
group size can be specified and groupshared memory can be allocated. This allows for highly parallelized execution 
and optimized memory usage. A key difference to the traditional pipeline is, that the Amplification Shader can 
invoke other compute shaders, create new tasks and dispatch none, one, or multiple mesh shader invocations. This 
makes the shader extremely flexible and capable of providing the same results as the Geometry Shader did.\\

[@TODO: Add picture of meshlets]
\noindent
The Mesh Shader output can be defined individually but usually it creates a small patch consisting of a small 
number of triangles, called \emph{Meshlet}. This new approach to operating on vertices allows for a new interpretation 
of mesh data. The primitive topology is not anymore constrained to triangles, triangle fans and triangle strips, 
but can be individually specified by the developers.
Meshlets are computed by thread groups which usually fetch the mesh data from the given buffers and apply 
transformations and arbitrary operations. This new way of processing geometry is not only more efficient in many cases 
and reduces the memory bandwidth, but also allows for per-meshlet operations \cite{Kubisch2020}. This is a key 
advantage, since per-instance geometry can now be processed in parallel and still be individually discarded or post 
processed. There are several other advantages over the traditional Vertex Shaded Pipeline, which we will not list to 
the full degree. More details are provided in \cite{Kubisch2020}.\\


\subsection{Meshlet Culling} \label{subsec-meshlet-culling}

One major innovation which is enabled by the use of Mesh Shading is to cull on a per-primitve (per-meshlet) basis. 
This allows for more granular culling of high density geometry even within instances. Prior to the implementation 
of meshlets, culling had to be done on a per-instance basis (or for backface culling on a per triangle basis), 
ommiting whole instances depending on their position in space or their visibility from the point of view of the camera.\\

\noindent
But although the Mesh Shading Pipeline was first introduced in 2018, the idea of triangle clusters goes back even further.
One example of this \ac{GPU} Driven Rendering technique is the 2014 title \emph{Assassin's Creed Unity} \cite{Ubisoft2014}.
\cite{Aaltonen2015} provides an overview over multiple cluster related optimization techniques, implementing a highly 
parallelized rendering pipeline before Mesh Shading became part of the official Rendering \ac{API}s.
For their \ac{GPU} culling algorithms they refer to the prior work of Hill \cite{Hill11} and Greene \cite{Greene93} which 
provides the basis for some of the implementation in our case study.


\subsubsection{Meshlet Backface Culling} \label{subsubsec-meshlet-backface-culling}

[@TODO: Rewrite in context of Backface culling above]
Backface culling refers to the discard of triangles, which face away from the camera. These triangles can usually be omitted 
since they will not be seen by the camera anyway. Except from a few exceptions, backface culling can be applied to the 
pipeline configuration safely. Meshlet backface culling refers to the same principle. In this case, not individual triangles  
are culled but whole meshlets are discarded if they are facing away from the camera. Practical algorithms to determine meshlet 
orientation are [@TODO: cite algos].\\ %\cite{} \cite{} \cite{}

%\noindent
%Meshlet backface culling can be more efficient because adjacent triangles usually face in similar directions.

\subsubsection{Meshlet View Frustum Culling} \label{subsubsec-meshlet-view-frustum-culling}

\subsubsection{Meshlet Occlusion Culling} \label{subsubsec-meshlet-occlusion-culling}